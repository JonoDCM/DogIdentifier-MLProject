{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ML Dog Identifier"
      ],
      "metadata": {
        "id": "GQw0LwHlB0qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our research questions are:\n",
        "\n",
        "Can our custom CNN match ResNet-18 in classification accuracy?\n",
        "\n",
        "\n",
        "Which dog breeds are most often misclassified, and why?\n",
        "\n",
        "\n",
        "How does image background—indoor versus outdoor—affect model performance?\n",
        "\n",
        "\n",
        "Can we prune the model to reduce inference time without significant accuracy loss?\n",
        "\n",
        "\n",
        "Finally, is real-time inference achievable on mobile hardware?\"\n"
      ],
      "metadata": {
        "id": "NWZknau8B5aB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng6U_MZFHbjM"
      },
      "outputs": [],
      "source": [
        "!pip install -q tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, models\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "gSlD7bb4npA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "P9gCZZ5GoybK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0_a_cDEyo3Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/MyDrive/stanford-dogs/images/Images\""
      ],
      "metadata": {
        "id": "ERq4GBhX5qvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List all breed folders\n",
        "classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
        "print(f\"Found {len(classes)} breeds:\\n\", classes)\n",
        "\n",
        "# Count images per breed\n",
        "counts = {cls: len(os.listdir(os.path.join(data_dir, cls))) for cls in classes}\n",
        "print(\"\\nSample counts:\", dict(list(counts.items())))\n"
      ],
      "metadata": {
        "id": "JA0H0GU76Co9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, random\n",
        "from pathlib import Path\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Split ratios\n",
        "train_ratio, val_ratio = 0.8, 0.1\n",
        "test_ratio = 1.0 - train_ratio - val_ratio\n",
        "\n",
        "# Where to write the split folders\n",
        "base_split = Path(\"/content/stanford-dogs-splits\")\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    (base_split / split).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Perform split\n",
        "for cls in classes:\n",
        "    imgs = list((Path(data_dir) / cls).glob(\"*.jpg\"))\n",
        "    random.shuffle(imgs)\n",
        "    n = len(imgs)\n",
        "    n_train = int(train_ratio * n)\n",
        "    n_val = int(val_ratio * n)\n",
        "\n",
        "    splits = {\n",
        "        \"train\": imgs[:n_train],\n",
        "        \"val\": imgs[n_train:n_train + n_val],\n",
        "        \"test\": imgs[n_train + n_val:]\n",
        "    }\n",
        "\n",
        "    for split_name, files in splits.items():\n",
        "        target_dir = base_split / split_name / cls\n",
        "        target_dir.mkdir(parents=True, exist_ok=True)\n",
        "        for f in files:\n",
        "            shutil.copy(f, target_dir / f.name)\n",
        "\n",
        "print(\"Done! Check `/content/stanford-dogs-splits/{train,val,test}`\")"
      ],
      "metadata": {
        "id": "L2p1Iq2b8IKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# We resize images to 224×224 (standard for ResNet), convert to tensor, and normalize w.r.t. ImageNet stats\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std =[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Point to your split folders\n",
        "base_split = \"/content/stanford-dogs-splits\"\n",
        "train_dir = f\"{base_split}/train\"\n",
        "val_dir   = f\"{base_split}/val\"\n",
        "test_dir  = f\"{base_split}/test\"\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ImageFolder(root=train_dir, transform=transform)\n",
        "val_dataset   = ImageFolder(root=val_dir,   transform=transform)\n",
        "test_dataset  = ImageFolder(root=test_dir,  transform=transform)"
      ],
      "metadata": {
        "id": "F5zDvmonIdWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "QnJv-GifLMeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# Get one batch\n",
        "images, labels = next(iter(train_loader))\n",
        "class_names = train_dataset.classes\n",
        "\n",
        "# Unnormalize and plot\n",
        "img_grid = make_grid(images[:16], nrow=4, padding=2)\n",
        "np_img = img_grid.numpy().transpose((1, 2, 0))\n",
        "np_img = np.clip((np_img * [0.229,0.224,0.225] + [0.485,0.456,0.406]), 0, 1)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(np_img)\n",
        "plt.title([class_names[i] for i in labels[:16]])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J-vMTgq3LQG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q http://places2.csail.mit.edu/models_places365/resnet18_places365.pth.tar"
      ],
      "metadata": {
        "id": "GqaF2XrAL29T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models as tv_models\n",
        "\n",
        "places_model = tv_models.resnet18(num_classes=365)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "checkpoint = torch.load('resnet18_places365.pth.tar', map_location=device)\n",
        "\n",
        "state_dict = {k.replace('module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
        "places_model.load_state_dict(state_dict)\n",
        "\n",
        "places_model.eval().to(device)\n",
        "\n",
        "print(\"Places365 model loaded successfully.\")"
      ],
      "metadata": {
        "id": "5bUaqxG7Lh0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt\n",
        "\n",
        "!wget -q https://raw.githubusercontent.com/csailvision/places365/master/IO_places365.txt"
      ],
      "metadata": {
        "id": "MGhK1uuDMVxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scene_classes = []\n",
        "with open('categories_places365.txt') as f:\n",
        "    for line in f:\n",
        "        scene_classes.append(line.strip().split(' ')[0])\n",
        "\n",
        "io_labels = []\n",
        "with open('IO_places365.txt') as f:\n",
        "    for line in f:\n",
        "        io_labels.append(int(line.strip().split()[-1]))"
      ],
      "metadata": {
        "id": "wJEIdBLBMZQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "places_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std =[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "hNWy4vWEMhDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "test_dir = \"/content/stanford-dogs-splits/test\"\n",
        "\n",
        "records = []\n",
        "for breed in classes:\n",
        "    breed_dir = Path(test_dir) / breed\n",
        "    for img_path in breed_dir.glob(\"*.jpg\"):\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        inp = places_transform(img).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = places_model(inp)\n",
        "        idx = torch.argmax(logits, dim=1).item()\n",
        "        bg = \"outdoor\" if io_labels[idx] == 1 else \"indoor\"\n",
        "        records.append({\n",
        "            \"path\": str(img_path),\n",
        "            \"breed\": breed,\n",
        "            \"background\": bg\n",
        "        })\n",
        "\n",
        "df_bg = pd.DataFrame(records)\n",
        "df_bg.head()"
      ],
      "metadata": {
        "id": "PwPoE8v7MjNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),        # random crop and resize\n",
        "    transforms.RandomHorizontalFlip(),        # flip half the images\n",
        "    transforms.ColorJitter(brightness=0.2,    # random color adjustments\n",
        "                           contrast=0.2,\n",
        "                           saturation=0.2,\n",
        "                           hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std =[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std =[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "4wVh4sSuM2jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n",
        "val_dataset   = ImageFolder(root=val_dir,   transform=val_test_transform)\n",
        "test_dataset  = ImageFolder(root=test_dir,  transform=val_test_transform)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "QWz-MiiMM748"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "images, labels = next(iter(train_loader))\n",
        "img_grid = make_grid(images[:16], nrow=4)\n",
        "np_img = img_grid.numpy().transpose((1,2,0))\n",
        "np_img = np.clip((np_img * [0.229,0.224,0.225] + [0.485,0.456,0.406]), 0,1)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(np_img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8TvE0uCrNAgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Training transforms with augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),        # random crop + resize\n",
        "    transforms.RandomHorizontalFlip(),        # flip half the images\n",
        "    transforms.ColorJitter(brightness=0.2,    # random color adjustments\n",
        "                           contrast=0.2,\n",
        "                           saturation=0.2,\n",
        "                           hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std =[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation/Test transforms (no augmentation)\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std =[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "KgFNi014NOZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Re-create datasets with new transforms\n",
        "train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n",
        "val_dataset   = ImageFolder(root=val_dir,   transform=val_test_transform)\n",
        "test_dataset  = ImageFolder(root=test_dir,  transform=val_test_transform)\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "mNc7AFaWNYaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# Grab a batch and show 16 augmented images\n",
        "images, labels = next(iter(train_loader))\n",
        "img_grid = make_grid(images[:16], nrow=4)\n",
        "np_img = img_grid.numpy().transpose((1,2,0))\n",
        "np_img = np.clip((np_img * [0.229,0.224,0.225] + [0.485,0.456,0.406]), 0,1)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(np_img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ilqfk_TVNaUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom CNN"
      ],
      "metadata": {
        "id": "ZlB2hbWkOCjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(train_dataset.classes)\n",
        "\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SmallCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "small_cnn = SmallCNN(num_classes).to(device)"
      ],
      "metadata": {
        "id": "RdB3EyoDO69A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "import torch.nn as nn\n",
        "\n",
        "weights = ResNet18_Weights.DEFAULT\n",
        "\n",
        "resnet18 = resnet18(weights=weights)\n",
        "\n",
        "in_features = resnet18.fc.in_features\n",
        "resnet18.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "for name, param in resnet18.named_parameters():\n",
        "    if \"fc\" not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "resnet18 = resnet18.to(device)"
      ],
      "metadata": {
        "id": "TFbwT1LmNpw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Begin Training"
      ],
      "metadata": {
        "id": "6oR9hBD4PN1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data).item()\n",
        "        total += inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc  = running_corrects / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, loader, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc=\"Validate\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).item()\n",
        "            total += inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc  = running_corrects / total\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "UYiprqEbO4X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "optimizer_small = optim.AdamW(small_cnn.parameters(),\n",
        "                              lr=1e-3, weight_decay=1e-4)\n",
        "scheduler_small = CosineAnnealingLR(optimizer_small, T_max=10)\n",
        "\n",
        "optimizer_resnet = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, resnet18.parameters()),\n",
        "    lr=1e-3, weight_decay=1e-4\n",
        ")\n",
        "scheduler_resnet = CosineAnnealingLR(optimizer_resnet, T_max=10)\n",
        "\n",
        "def fit_model(model, train_loader, val_loader, optimizer, scheduler, device, epochs=10):\n",
        "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "    for epoch in range(1, epochs+1):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
        "        val_loss,   val_acc   = validate(model,   val_loader,   device)\n",
        "        scheduler.step()\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} | \"\n",
        "              f\"Train: {train_loss:.4f}, {train_acc:.4f} | \"\n",
        "              f\"Val:   {val_loss:.4f}, {val_acc:.4f}\")\n",
        "    return history\n",
        "\n",
        "hist_small = fit_model(small_cnn, train_loader, val_loader,\n",
        "                       optimizer_small, scheduler_small,\n",
        "                       device, epochs=10)\n",
        "\n",
        "hist_resnet = fit_model(resnet18, train_loader, val_loader,\n",
        "                        optimizer_resnet, scheduler_resnet,\n",
        "                        device, epochs=10)\n"
      ],
      "metadata": {
        "id": "cANYzBpqPSDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(hist, title):\n",
        "    epochs = range(1, len(hist[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochs, hist[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, hist[\"val_loss\"],   label=\"Val Loss\")\n",
        "    plt.title(f\"{title} — Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochs, hist[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(epochs, hist[\"val_acc\"],   label=\"Val Acc\")\n",
        "    plt.title(f\"{title} — Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_history(hist_small, \"SmallCNN\")\n",
        "\n",
        "plot_history(hist_resnet, \"ResNet-18\")"
      ],
      "metadata": {
        "id": "5r1JnD1wPYOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate"
      ],
      "metadata": {
        "id": "VX99p3RPViNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate SmallCNN\n",
        "test_loss_small, test_acc_small = validate(small_cnn, test_loader, device)\n",
        "print(f\"SmallCNN Test — Loss: {test_loss_small:.4f}, Accuracy: {test_acc_small:.4f}\")\n",
        "\n",
        "# Evaluate ResNet-18\n",
        "test_loss_resnet, test_acc_resnet = validate(resnet18, test_loader, device)\n",
        "print(f\"ResNet-18 Test — Loss: {test_loss_resnet:.4f}, Accuracy: {test_acc_resnet:.4f}\")"
      ],
      "metadata": {
        "id": "Je7lfcNwVjub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Gather all predictions & true labels\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "resnet18.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = resnet18(inputs)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "class_names = test_dataset.classes\n",
        "\n",
        "# Convert to DataFrame for readability\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "df_cm.head()"
      ],
      "metadata": {
        "id": "Qht5xAU4VuhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero out the diagonal to ignore correct predictions\n",
        "cm_no_diag = cm.copy()\n",
        "np.fill_diagonal(cm_no_diag, 0)\n",
        "\n",
        "N = 10\n",
        "flat = cm_no_diag.flatten()\n",
        "top_idxs = flat.argsort()[::-1][:N]\n",
        "rows = top_idxs // cm_no_diag.shape[1]\n",
        "cols = top_idxs % cm_no_diag.shape[1]\n",
        "counts = flat[top_idxs]\n",
        "\n",
        "misclassifications = pd.DataFrame({\n",
        "    \"Predicted\": [class_names[c] for c in cols],\n",
        "    \"Actual\":    [class_names[r] for r in rows],\n",
        "    \"Count\":     counts\n",
        "})\n",
        "\n",
        "misclassifications"
      ],
      "metadata": {
        "id": "YmIJnA3lVxmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "paths, true_idxs = zip(*test_dataset.samples)\n",
        "\n",
        "df_results = pd.DataFrame({\n",
        "    \"path\": paths,\n",
        "    \"actual\": [test_dataset.classes[i] for i in all_labels],\n",
        "    \"predicted\": [test_dataset.classes[i] for i in all_preds]\n",
        "})\n",
        "\n",
        "df_results.head()"
      ],
      "metadata": {
        "id": "R4RX2rhtWA0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = df_results.merge(df_bg[[\"path\", \"background\"]], on=\"path\")\n",
        "\n",
        "# Quick peek\n",
        "df_merged.head()"
      ],
      "metadata": {
        "id": "itQlzMSiWG2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged[\"correct\"] = (df_merged[\"predicted\"] == df_merged[\"actual\"]).astype(int)\n",
        "\n",
        "accuracy_by_bg = df_merged.groupby(\"background\")[\"correct\"].mean().reset_index()\n",
        "accuracy_by_bg.columns = [\"background\", \"accuracy\"]\n",
        "\n",
        "print(accuracy_by_bg)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(accuracy_by_bg[\"background\"], accuracy_by_bg[\"accuracy\"])\n",
        "plt.ylim(0,1)\n",
        "plt.title(\"Test Accuracy by Background Type\")\n",
        "plt.xlabel(\"Background\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HCOY-bOgWKeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Ensure model is in eval mode\n",
        "resnet18.eval()\n",
        "\n",
        "# Measure inference time over test set\n",
        "start_time = time.perf_counter()\n",
        "running_corrects = 0\n",
        "total_images = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Baseline Inference\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = resnet18(inputs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_images += inputs.size(0)\n",
        "\n",
        "end_time = time.perf_counter()\n",
        "\n",
        "baseline_acc  = running_corrects / total_images\n",
        "baseline_time = (end_time - start_time) / total_images  # seconds per image\n",
        "\n",
        "print(f\"Baseline ResNet-18 — Accuracy: {baseline_acc:.4f}, \"\n",
        "      f\"Avg Inference Time: {baseline_time*1000:.2f} ms/image\")"
      ],
      "metadata": {
        "id": "i2740INkWPU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pruning"
      ],
      "metadata": {
        "id": "LbYRcje9TPXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn as nn\n",
        "\n",
        "# Prune 30% of weights in each Conv2d and Linear layer\n",
        "for module in resnet18.modules():\n",
        "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "        prune.l1_unstructured(module, name=\"weight\", amount=0.3)\n",
        "        # Make pruning permanent\n",
        "        prune.remove(module, \"weight\")\n",
        "\n",
        "print(\"Applied 30% L1 unstructured pruning to all Conv2d and Linear layers.\")"
      ],
      "metadata": {
        "id": "RJGJK-JfWko6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure pruned model is in eval mode\n",
        "resnet18.eval()\n",
        "\n",
        "# Measure inference time over test set again\n",
        "start_time = time.perf_counter()\n",
        "running_corrects_pruned = 0\n",
        "total_images = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Pruned Inference\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = resnet18(inputs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        running_corrects_pruned += torch.sum(preds == labels).item()\n",
        "        total_images += inputs.size(0)\n",
        "\n",
        "end_time = time.perf_counter()\n",
        "\n",
        "pruned_acc  = running_corrects_pruned / total_images\n",
        "pruned_time = (end_time - start_time) / total_images  # seconds per image\n",
        "\n",
        "print(f\"Pruned ResNet-18 — Accuracy: {pruned_acc:.4f}, \"\n",
        "      f\"Avg Inference Time: {pruned_time*1000:.2f} ms/image\")"
      ],
      "metadata": {
        "id": "wRVPJoT8Wm9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_prune = pd.DataFrame({\n",
        "    \"Model\": [\"Baseline\", \"Pruned (30%)\"],\n",
        "    \"Accuracy\": [baseline_acc, pruned_acc],\n",
        "    \"Time_ms_per_image\": [baseline_time*1000, pruned_time*1000]\n",
        "})\n",
        "df_prune"
      ],
      "metadata": {
        "id": "NdorMk7DWpp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Move the pruned ResNet-18 to CPU and set eval mode\n",
        "resnet18_cpu = resnet18.to('cpu')\n",
        "resnet18_cpu.eval()\n",
        "\n",
        "# Create example input matching our model’s expected size\n",
        "example_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Trace the model\n",
        "traced_model = torch.jit.trace(resnet18_cpu, example_input)\n",
        "\n",
        "# Save the TorchScript module\n",
        "traced_model.save(\"resnet18_pruned_mobile.pt\")\n",
        "\n",
        "print(\"Model exported to TorchScript: resnet18_pruned_mobile.pt\")"
      ],
      "metadata": {
        "id": "hN0YjFdqWz6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Load the TorchScript model\n",
        "mobile_model = torch.jit.load(\"resnet18_pruned_mobile.pt\")\n",
        "mobile_model.eval()\n",
        "mobile_model.to('cpu')\n",
        "\n",
        "latencies = []\n",
        "\n",
        "# Iterate through test images batch by batch\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in tqdm(test_loader, desc=\"Mobile CPU Inference\"):\n",
        "        # inputs: [batch_size, 3, 224, 224]\n",
        "        for img in inputs:\n",
        "            img = img.unsqueeze(0).to('cpu')  # make it [1,3,224,224]\n",
        "            start = time.perf_counter()\n",
        "            _ = mobile_model(img)\n",
        "            end = time.perf_counter()\n",
        "            latencies.append((end - start) * 1000)  # convert to milliseconds\n",
        "\n",
        "# Compute average latency\n",
        "avg_latency = sum(latencies) / len(latencies)\n",
        "print(f\"Average CPU inference latency: {avg_latency:.2f} ms/image\")"
      ],
      "metadata": {
        "id": "i-OZ_7CmW6K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1) Test Accuracies\n",
        "df_accuracy = pd.DataFrame({\n",
        "    \"Model\": [\"SmallCNN\", \"ResNet-18\"],\n",
        "    \"Test Accuracy\": [test_acc_small, test_acc_resnet]\n",
        "})\n",
        "\n",
        "df_latency = pd.DataFrame({\n",
        "    \"Model\": [\"CPU inference (ms/image)\"],\n",
        "    \"Value\": [avg_latency]\n",
        "})\n",
        "\n",
        "# Display all tables\n",
        "print(\"=== Test Accuracy Comparison ===\")\n",
        "display(df_accuracy)\n",
        "\n",
        "print(\"\\n=== Top Misclassified Breed Pairs ===\")\n",
        "display(misclassifications)\n",
        "\n",
        "print(\"\\n=== Accuracy by Background ===\")\n",
        "display(accuracy_by_bg)\n",
        "\n",
        "print(\"\\n=== Pruning Trade-Off ===\")\n",
        "display(df_prune)\n",
        "\n",
        "print(\"\\n=== Mobile CPU Latency ===\")\n",
        "display(df_latency)"
      ],
      "metadata": {
        "id": "LcXcbQooXqIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(df_accuracy[\"Model\"], df_accuracy[\"Test Accuracy\"])\n",
        "plt.ylim(0,1)\n",
        "plt.title(\"Test Accuracy: SmallCNN vs ResNet-18\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vRxXMScvXxR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.bar(accuracy_by_bg[\"background\"], accuracy_by_bg[\"accuracy\"])\n",
        "plt.ylim(0,1)\n",
        "plt.title(\"Test Accuracy by Image Background\")\n",
        "plt.xlabel(\"Background\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q1a6IbLcaCVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.bar(df_prune[\"Model\"], df_prune[\"Accuracy\"])\n",
        "plt.ylim(0,1)\n",
        "plt.title(\"Accuracy Before vs After Pruning\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UNNXEIOjX62w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.bar(df_prune[\"Model\"], df_prune[\"Time_ms_per_image\"])\n",
        "plt.title(\"Inference Time Before vs After Pruning\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Time (ms per image)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qw_Oyhe8Z7fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "short_names = [name.split('-', 1)[1] for name in test_dataset.classes]\n",
        "\n",
        "report = classification_report(all_labels, all_preds, target_names=short_names)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "qRnYk3VGYiIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare short names mapping\n",
        "short_names = [name.split('-', 1)[1] for name in test_dataset.classes]\n",
        "\n",
        "# Build list of (path, actual_idx, pred_idx)\n",
        "paths = [p for p,_ in test_dataset.samples]\n",
        "pairs = list(zip(paths, all_labels, all_preds))\n",
        "\n",
        "# Split correct and incorrect\n",
        "correct_samples   = [(p,a,pr) for p,a,pr in pairs if a==pr]\n",
        "incorrect_samples = [(p,a,pr) for p,a,pr in pairs if a!=pr]\n",
        "\n",
        "# Randomly pick 3 of each\n",
        "random.seed(42)\n",
        "sample_correct   = random.sample(correct_samples,   3)\n",
        "sample_incorrect = random.sample(incorrect_samples, 3)\n",
        "\n",
        "def display_grid(samples, title):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    for i, (path, actual, pred) in enumerate(samples):\n",
        "        img = plt.imread(path)\n",
        "        plt.subplot(1, len(samples), i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"Actual: {short_names[actual]}\\nPred: {short_names[pred]}\")\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "display_grid(sample_correct,   \"3 Correct Predictions\")\n",
        "display_grid(sample_incorrect, \"3 Incorrect Predictions\")"
      ],
      "metadata": {
        "id": "zvfvL0Akap6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}